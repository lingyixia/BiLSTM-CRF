{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "\n",
    "def makeVocabulary(originPath='data/train.txt', vocabPath='dataProcess'):\n",
    "    if os.path.exists(os.path.join(vocabPath, 'vocab.txt')) and os.path.exists(os.path.join(vocabPath, 'taged.txt')):\n",
    "        return getVocabAndTags(vocabPath)\n",
    "    vocabSet = set()\n",
    "    tagedSet = set()\n",
    "    vocabSet.add('<UNK>')\n",
    "    vocabSet.add('<PAD>')\n",
    "    vocabSet.add('<START>')\n",
    "    vocabSet.add('<END>')\n",
    "    tagedSet.add('<START>')\n",
    "    tagedSet.add('<END>')\n",
    "    trainFile = open(originPath, encoding='utf-8')\n",
    "    for line in trainFile:\n",
    "        line = line.replace('\\n', '')\n",
    "        if line != '':\n",
    "            line = line.split(' ')\n",
    "            vocabSet.add(line[0])\n",
    "            tagedSet.add(line[1])\n",
    "    tagedSet.remove('O')\n",
    "    tagedList = list(tagedSet)\n",
    "    tagedList.insert(0, 'O')\n",
    "    vocab2index = dict(zip(vocabSet, range(len(vocabSet))))\n",
    "    index2vocab = dict(zip(range(len(vocabSet)), vocabSet))\n",
    "    taged2index = dict(zip(tagedList, range(len(tagedList))))\n",
    "    index2taged = dict(zip(range(len(tagedList)), tagedList))\n",
    "    vocabDic = {'vocab2index': vocab2index, 'index2vocab': index2vocab}\n",
    "    tagedDic = {'taged2index': taged2index, 'index2taged': index2taged}\n",
    "    with open(os.path.join(vocabPath, 'vocab.txt'), mode='w', encoding='utf-8') as writer:\n",
    "        json.dump(vocabDic, writer)\n",
    "    with open(os.path.join(vocabPath, 'taged.txt'), mode='w', encoding='utf-8') as writer:\n",
    "        json.dump(tagedDic, writer)\n",
    "    return vocab2index, index2vocab, taged2index, index2taged\n",
    "\n",
    "\n",
    "def getVocabAndTags(path='dataProcess'):\n",
    "    with open(os.path.join(path, 'vocab.txt'), encoding='utf-8') as reader:\n",
    "        vocabDic = json.load(reader)\n",
    "        vocab2index = vocabDic['vocab2index']\n",
    "        index2vocab = vocabDic['index2vocab']\n",
    "        index2vocab = dict(zip(map(int, index2vocab.keys()), index2vocab.values()))\n",
    "    with open(os.path.join(path, 'taged.txt'), encoding='utf-8') as reader:\n",
    "        tagedDic = json.load(reader)\n",
    "        taged2index = tagedDic['taged2index']\n",
    "        index2taged = tagedDic['index2taged']\n",
    "        index2taged = dict(zip(map(int, index2taged.keys()), index2taged.values()))\n",
    "    return vocab2index, index2vocab, taged2index, index2taged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, vocab2index, index2vocab, taged2index, index2taged, maxLength):\n",
    "        self.__vocab2index = vocab2index\n",
    "        self.__index2vocab = index2vocab\n",
    "        self.__taged2index = taged2index\n",
    "        self.__index2taged = index2taged\n",
    "        self.__maxLength = maxLength\n",
    "\n",
    "    def __getData(self, inputFile):\n",
    "        sentences = list()\n",
    "        sentenceLengths = list()\n",
    "        tages = list()\n",
    "        with open(inputFile, encoding='utf-8') as reader:\n",
    "            sentence = list()\n",
    "            tag = list()\n",
    "            for line in reader:\n",
    "                line = line.replace('\\n', '')\n",
    "                if line == '':\n",
    "                    sentence = [self.__vocab2index['<START>']] + sentence\n",
    "                    tag = [self.__taged2index['<START>']] + tag\n",
    "                    sentence.append(self.__vocab2index['<END>'])\n",
    "                    tag.append(self.__taged2index['<END>'])\n",
    "                    sentenceLengths.append(len(sentence))\n",
    "                    sentences.append(\n",
    "                        np.asarray(\n",
    "                            sentence.copy() + (self.__maxLength - len(sentence)) * [self.__vocab2index['<PAD>']]))\n",
    "                    tages.append(\n",
    "                        np.asarray(tag.copy() + (self.__maxLength - len(sentence)) * [self.__taged2index['O']]))\n",
    "                    sentence.clear()\n",
    "                    tag.clear()\n",
    "                else:\n",
    "                    thisWord = line.split(' ')[0]\n",
    "                    thisTag = line.split(' ')[1]\n",
    "                    sentence.append(self.__vocab2index.get(thisWord, self.__vocab2index['<UNK>']))\n",
    "                    tag.append(self.__taged2index[thisTag])\n",
    "        return np.asarray(sentences), np.asarray(tages), np.asarray(sentenceLengths)\n",
    "\n",
    "    def input_fn(self, inputFile, batchSize, ifShuffleAndRepeat=True):\n",
    "        sentences, tages, sentenceLengths = self.__getData(inputFile)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((sentences, sentenceLengths, tages))\n",
    "        if ifShuffleAndRepeat:\n",
    "            dataset = dataset.shuffle(1000)\n",
    "            dataset = dataset.repeat(25)\n",
    "        dataset = dataset.batch(batchSize)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        sentences, sentenceLengths, tag = iterator.get_next()\n",
    "        sentences = {'sentences': sentences, 'sentenceLengths': sentenceLengths}\n",
    "        return sentences, tag\n",
    "\n",
    "    def indexToText(self, sentence, tag):\n",
    "        sentence = list(map(lambda x: self.__index2vocab[x], sentence))\n",
    "        tag = list(map(lambda x: self.__index2taged[x], tag))\n",
    "        return sentence, tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.crf as crf\n",
    "from tf_metrics import precision, recall, f1\n",
    "from tensorflow.contrib import keras\n",
    "\n",
    "\n",
    "class BiLSTMCrf(object):\n",
    "    def __init__(self, inputX, inputY, sentenceLengths, numClasses, vocabSize, embeddingSize,\n",
    "                 hiddenSize, learnRate, maxLength, l2_reg_lambda, dropout_keep_prob, crf):\n",
    "        self.numClasses = numClasses\n",
    "        self.vocabSize = vocabSize\n",
    "        self.embeddingSize = embeddingSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.learnRate = learnRate\n",
    "        self.maxLength = maxLength\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.inputX = inputX\n",
    "        self.inputY = inputY\n",
    "        self.sentenceLengths = sentenceLengths\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.crf = crf\n",
    "        self.__addEmbeddingLayer()\n",
    "        self.__addBiLSTMLayer()\n",
    "\n",
    "    def __addEmbeddingLayer(self):\n",
    "        with tf.name_scope('embeddingLayer'):\n",
    "            embedding = tf.get_variable(name='embedding', shape=[self.vocabSize, self.embeddingSize],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddingInput = tf.nn.embedding_lookup(embedding, self.inputX)\n",
    "            self.embeddingInput = tf.nn.dropout(embeddingInput, rate=1 - self.dropout_keep_prob)\n",
    "\n",
    "    def __addBiLSTMLayer(self):\n",
    "        with tf.name_scope('BiLSTMLayer'):\n",
    "            lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=self.hiddenSize),\n",
    "                output_keep_prob=self.dropout_keep_prob)\n",
    "            lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=self.hiddenSize),\n",
    "                output_keep_prob=self.dropout_keep_prob)\n",
    "            # keras.layers.Bidirectional(\n",
    "            #     keras.layers.LSTM(units=self.hiddenSize, dropout=1 - self.dropout_keep_prob)).apply(\n",
    "            #     inputs=self.embeddingInput, mask=)\n",
    "            bidOutput, bidCurrent_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "                                                                          cell_bw=lstm_bw_cell,\n",
    "                                                                          sequence_length=self.sentenceLengths,\n",
    "                                                                          inputs=self.embeddingInput,\n",
    "                                                                          dtype=tf.float32)\n",
    "            BiLSTMOutput = tf.concat(bidOutput, axis=-1)\n",
    "            self.BiLSTMOutput = tf.nn.dropout(BiLSTMOutput, rate=1 - self.dropout_keep_prob)\n",
    "\n",
    "    def __addBiLSTMOutPutDenseLayer(self):\n",
    "        with tf.name_scope('BiLSTMOutputDenseLayer'):\n",
    "            l2_regularizer = tf.contrib.layers.l2_regularizer(scale=self.l2_reg_lambda)  # 获取正则项\n",
    "            self.bilstmDenseOutput = keras.layers.Dense(units=self.numClasses,\n",
    "                                                        activation=keras.activations.relu,\n",
    "                                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                        kernel_regularizer=l2_regularizer)(self.BiLSTMOutput)\n",
    "            # self.bilstmDenseOutput = tf.layers.dense(inputs=self.BiLSTMOutput,\n",
    "            #                                          units=self.numClasses,\n",
    "            #                                          activation=tf.nn.relu,\n",
    "            #                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            #                                          kernel_regularizer=l2_regularizer)\n",
    "            self.sequence = tf.argmax(self.bilstmDenseOutput, axis=-1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.inputY, logits=self.bilstmDenseOutput)\n",
    "            mask = tf.sequence_mask(self.sentenceLengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            self.l2_loss = tf.losses.get_regularization_loss()  # 使用get_regularization_loss函数获取定义的全\n",
    "            self.loss += self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "    def __addCrfLayer(self):\n",
    "        with tf.name_scope('CRFLayer'):\n",
    "            self.transitionParams = tf.get_variable(\"transitions\", shape=[self.numClasses, self.numClasses],\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            logLikelihood, self.transitionParams = crf.crf_log_likelihood(self.bilstmDenseOutput,\n",
    "                                                                          self.inputY,\n",
    "                                                                          self.sentenceLengths,\n",
    "                                                                          transition_params=self.transitionParams)\n",
    "            self.sequence, _ = crf.crf_decode(self.bilstmDenseOutput,\n",
    "                                              self.transitionParams,\n",
    "                                              self.sentenceLengths)\n",
    "            self.loss = tf.reduce_mean(-logLikelihood)\n",
    "            self.loss += self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "    def getResult(self, mode):\n",
    "        self.__addBiLSTMOutPutDenseLayer()\n",
    "        if self.crf:\n",
    "            self.__addCrfLayer()\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return self.sequence\n",
    "        else:\n",
    "            weights = tf.sequence_mask(self.sentenceLengths, maxlen=self.maxLength, dtype=tf.int32)\n",
    "            metrics = {\n",
    "                'acc': tf.metrics.accuracy(labels=self.inputY, predictions=self.sequence, weights=weights),\n",
    "                'precision': precision(labels=self.inputY, predictions=self.sequence, num_classes=self.numClasses,\n",
    "                                       pos_indices=[1, 3, 4, 6, 7, 8], weights=weights),\n",
    "                'recall': recall(labels=self.inputY, predictions=self.sequence, num_classes=self.numClasses,\n",
    "                                 pos_indices=[1, 3, 4, 6, 7, 8], weights=weights),\n",
    "                'f1': f1(labels=self.inputY, predictions=self.sequence, num_classes=self.numClasses,\n",
    "                         pos_indices=[1, 3, 4, 6, 7, 8], weights=weights)\n",
    "            }\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                for metric_name, op in metrics.items():\n",
    "                    tf.summary.scalar(metric_name, op[1])\n",
    "                learnRate = tf.train.exponential_decay(self.learnRate, tf.train.get_global_step(), 500, 0.98,\n",
    "                                                       staircase=True)\n",
    "                optimizer = tf.train.AdamOptimizer(learnRate)\n",
    "                self.train_op = optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n",
    "                return self.loss, self.train_op\n",
    "            else:\n",
    "                return self.loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb37cd0da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-12-657faa069cce>:35: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-657faa069cce>:47: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py:286: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-0\n",
      "WARNING:tensorflow:From /Users/chenfeiyu01/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./model/model.ckpt.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c39f62597f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     train_inputFun = functools.partial(dataGenerator.input_fn, os.path.join(FLAGS.dataDir, 'train.txt'),\n\u001b[1;32m     46\u001b[0m                                        batchSize=FLAGS.batchSize)\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputFun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     eval_inputFun = functools.partial(dataGenerator.input_fn, os.path.join(FLAGS.dataDir, 'dev.txt'),\n\u001b[1;32m     49\u001b[0m                                       batchSize=FLAGS.batchSize, ifShuffleAndRepeat=False)\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1157\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1405\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tools/Miniconda/miniconda3/envs/NN3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, functools, argparse\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='BiLSTM-CRF超参数设置')\n",
    "parser.add_argument('--maxLength', type=int, default=105, help='序列最大长度')\n",
    "parser.add_argument(\"--embeddingSize\", type=int, default=64, help='字向量维度')\n",
    "parser.add_argument(\"--hiddenSize\", type=int, default=128, help='LSTM隐藏层维度')\n",
    "parser.add_argument('--learnRate', type=float, default=0.1, help='学习率设置')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5, help='dropout keep prob')\n",
    "parser.add_argument(\"--dataDir\", type=str, default='data', help='数据路径')\n",
    "parser.add_argument(\"--batchSize\", type=int, default=50, help='batchSize')\n",
    "parser.add_argument(\"--crf\", type=bool, default=True, help='是否使用crf')\n",
    "parser.add_argument('--l2_reg_lambda', type=float, default=0.1, help='l2正则项系数')\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    inputX = features['sentences']\n",
    "    sentenceLengths = features['sentenceLengths']\n",
    "    model = BiLSTMCrf(inputX, labels, sentenceLengths, params['numClasses'], params['vocabSize'],\n",
    "                      FLAGS.embeddingSize, FLAGS.hiddenSize, FLAGS.learnRate, FLAGS.maxLength, FLAGS.l2_reg_lambda,\n",
    "                      FLAGS.dropout if mode == tf.estimator.ModeKeys.TRAIN else 1.0, FLAGS.crf)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        loss, train_op = model.getResult(mode)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss, metrics = model.getResult(mode)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=metrics)\n",
    "    else:\n",
    "        sequence = model.getResult(mode)\n",
    "        predictions = {'sentence': inputX,\n",
    "                       'tags': sequence}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FLAGS = parser.parse_known_args()[0]\n",
    "    vocab2index, index2vocab, taged2index, index2taged = makeVocabulary()\n",
    "    params = {'numClasses': len(index2taged),\n",
    "              'vocabSize': len(index2vocab)}\n",
    "    dataGenerator = DataGenerator(vocab2index, index2vocab, taged2index, index2taged, FLAGS.maxLength)\n",
    "    model = tf.estimator.Estimator(model_fn=model_fn, params=params, model_dir=\"./model/\")\n",
    "    train_inputFun = functools.partial(dataGenerator.input_fn, os.path.join(FLAGS.dataDir, 'train.txt'),\n",
    "                                       batchSize=FLAGS.batchSize)\n",
    "    model.train(train_inputFun)\n",
    "    eval_inputFun = functools.partial(dataGenerator.input_fn, os.path.join(FLAGS.dataDir, 'dev.txt'),\n",
    "                                      batchSize=FLAGS.batchSize, ifShuffleAndRepeat=False)\n",
    "    model.evaluate(eval_inputFun)\n",
    "    test_inputFun = functools.partial(dataGenerator.input_fn, os.path.join(FLAGS.dataDir, 'test.txt'),\n",
    "                                      batchSize=FLAGS.batchSize,\n",
    "                                      ifShuffleAndRepeat=False)\n",
    "    predictions = model.predict(test_inputFun)\n",
    "    for result in predictions:\n",
    "        sentence, tags = dataGenerator.indexToText(result['sentence'], result['tags'])\n",
    "        print(dict(zip(sentence, tags)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
